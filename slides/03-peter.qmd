# Wrangling your Data

```{r}
#| echo: false
#| message: false
library(readxl)
library(tidyverse)
library(janitor)
library(here)
library(gtsummary)
library(padr)
library(countdown)
library(emo)
```

## Where are We Now?
- We have done DEV (Data Evaluation and Validation) - more validation to be done
- Discussed the importance of preventing data errors at the source - data collection and entry
- Did Stage 1 cleaning - variable names, remove empty columns/rows, variable types/classes (characters in numeric, recoding factors, date madness), missingness, violations of tidy data principles (separate), added meaningful variable labels.

## Deciding on the Unit of Observation

-   In Prospective Data Collection, a patient arrives for a visit. All data collected are part of the same observation/visit.
    -   Alternatively, we can divide a visit into distinct observations, like blood pressure, a PHQ-9 depression questionnaire, and a hemoglobin measurement (all on the same date)
-   In retrospective data review, we can divide the observations up as we choose. For inpatient stays, we need to decide *a priori* on how to handle multiple observations of the same type (e.g., vitals q6h) in the same day.
    -   use the 0800 observation each day?
    -   use the daily average for SBP and DBP?
    -   use the max values each day?

## What is the Unit of Analysis?

-   While the unit of observation may be straightforward for outpatient visits, and complicated for inpatient stays, in the end we need to select a Unit of Analysis
-   This Unit of Analysis usually depends on the **Question** we want to ask

## Is the Unit of Analysis the Patient?

-   Did the patient die?
-   Did the patient have the outcome of colectomy?
-   Did the patient reach disease remission?

## Is the Unit of Analysis the Visit/Encounter?

-   Often these are within-patient outcomes
    -   Did the C-reactive protein improve from Week 0 to Week 8?
    -   Did the number of sickle cell crises/year decrease after CRISPR gene therapy?
    -   Did the endoscopic ulcer score decrease on the experimental therapy vs placebo?

## Reshaping your data with tidyr

::: panel-tabset
### The Problem

-   We often *enter* data by patient
-   Spreadsheets encourage us to enter longitudinal data as long rows (per patient)
-   We end up with **wide**, rather than **tall** data

### Wide Version of Data

```{r,  echo=FALSE, warning=FALSE, message=FALSE}
wide <- readxl::read_excel(here("data/messy_uc.xlsx"), sheet = 3, skip = 6) |>
  janitor::clean_names() |>
  janitor::remove_empty(which = c("rows", "cols")) |> 
  select(pat_id, treatment, ends_with("mes"), ends_with("bss"), ends_with("emo")) |> 
  mutate(across(3:8, as.numeric)) 

head(wide)
```

### Tall Version of Data

```{r, echo=FALSE}
wide |> 
  pivot_longer(cols = 3:8,
               names_to = "measure",
               values_to = "score")
```
:::

## Reshaping your data with tidyr

-   R (and most R functions) are vectorized to handle tall data
-   One small observation per row
-   Most analyses in R are easier with tall data

```{r, echo=FALSE}
wide |> 
  pivot_longer(cols = 3:8,
               names_to = "measure",
               values_to = "score")
```

## Pivoting Longer (common)

-   We need to 'pivot' data from wide to tall on the regular
-   This "lengthens" data, increasing the number of rows, and decreasing the number of columns
-   We will be looking at Visit Dates (Start vs End) and Measures

## Pivoting Longer

-   Arguments: data, cols, names_to, values_to, and many optional arguments
-   Details from the tidyverse help page are [here](https://tidyr.tidyverse.org/reference/pivot_longer.html)
-   **data** is your dataframe/tibble - you can pipe this in
-   **cols** = columns to pivot, as a vector of names, or by number, or selected with [tidyselect](https://tidyselect.r-lib.org) functions
-   **names_to** = A character vector specifying the new column or columns to create from the information stored in the column names of data specified by cols.
-   **values_to** = A string specifying the name of the column to create from the data stored in cell values.

## Pivoting Longer (Example)

Let's start with the wide version (selected columns from messy_uc)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
wide <- readxl::read_excel(here("data/messy_uc.xlsx"), sheet = 3, skip = 6) |>
  janitor::clean_names() |>
  janitor::remove_empty(which = c("rows", "cols")) |> 
  select(pat_id, treatment, ends_with("mes"), ends_with("bss"), ends_with("emo")) |> 
  mutate(across(3:8, as.numeric)) 

wide
```

-   Note that there are 30 rows, one per patient, with 6 measured quantities for each patient.

## Pivoting Longer (Example)

This is the tall version we want to end up with.

```{r, echo=FALSE}
wide |> 
  pivot_longer(cols = 3:8, 
               names_to = "measure",
               values_to = "score") 
```

-   Note that there now 180 rows (30\*6), with one row per observation measure.

## Doing the pivot_longer()

What values do we want for these key arguments in order to pivot_longer?

-   cols (which columns to pivot)
-   names_to (variable to store the names)
-   values_to (variable to store the values)

```{r, echo=FALSE}
wide
```

## Pivoting Longer In Action

::: panel-tabset

### Problem: Wide

```{r, echo=FALSE}
wide
```

### Code: pivot_longer

```{r, echo=TRUE, eval=FALSE}
wide |> 
  pivot_longer(
    cols = "start_mes":"end_emo", 
    # could also use 3:8
               names_to = "measure",
               values_to = "score")
```

### Result: Tall

```{r, echo=FALSE, eval=TRUE}
tall <- wide |> 
  pivot_longer(
    cols = "start_mes":"end_emo", 
               names_to = "measure",
               values_to = "score")

tall
```

-   Does this make sense so far?
-   Zoom reaction: "thumbs up" emoji `r knitr::asis_output("\U1F44D")` if yes
    -   "raised hand" emoji `r knitr::asis_output("\U270B")` if puzzled/questions 
    
:::

## One Minor Issue - Separation of measure



::: panel-tabset

### Problem

-   the "measure" column combines a timepoint and the measure
-   Needs to be separated.
-   You already know how to use *separate()*
-   Arguments
    -   col
    -   sep
    -   into

### Code

```{r, echo=TRUE, eval=FALSE}
tall |> 
  separate(col = "measure",
           sep = "_",
           into = c("timept", "measure"))
```

### Result

```{r, echo=FALSE, eval=TRUE}
tall |> 
  separate(col = "measure",
           sep = "_",
           into = c("timept", "measure"))
```

### Alternative within pivot_longer

-   You can do this *within* pivot_longer with one more argument (if you are *fancy*)

::: columns

::: {.column width="50%"}
```{r, echo=TRUE, eval=FALSE}
wide |> 
  pivot_longer(cols = 3:8,
    names_to = c("timept", "measure"),
    names_sep = "_",
    values_to = "score")

```

:::

::: {.column width="50%"}
```{r, echo=FALSE, eval=TRUE}
wide |> 
  pivot_longer(cols = 3:8,
    names_to = c("timept", "measure"),
    names_sep = "_",
    values_to = "score")

```
:::

:::

:::

## Pivoting Longer

-   Your Turn (Exercise) with endo_data
-   Measurements of Trans-Epithelial Electrical Resistance (TEER, the inverse of leakiness) in biopsies of 3 segments of intestine.
-   This could be affected by portal hypertension in pts with liver cirrhosis

```{r, echo=FALSE}
endo_data <- tibble::tribble(
  ~pat_id, ~portal_htn, ~duod_teer, ~ileal_teer, ~colon_teer,
  001, 1, 4.33, 14.57, 16.23,
  002, 0, 11.67, 15.99, 18.97,
  003, 1, 4.12, 13.77, 15.22,
  004, 1, 4.62, 16.37, 18.12,
  005, 0, 12.43, 15.84, 19.04,
  006, 0, 13.05, 16.23, 18.81,
  007, 0, 11.88, 15.72, 18.31,
  008, 1, 4.87, 16.59, 18.77,
  009, 1, 4.23, 15.04, 16.87,
  010, 0, 12.77, 16.73, 19.12
)
endo_data
```

## Pivoting Longer with endo_data

::: panel-tabset
### Dataset

```{r, echo=FALSE}
endo_data
```

### Arguments

-   What values do you want to use for:
    -   cols
    -   names_pattern = "(.+)\_teer"
    -   names_to
    -   values_to
-   Note that we are giving you the value for names_pattern, which means that we want to keep the characters of the name (of whatever length) before "\_teer"

### Code

-   Fill in the blanks to pivot this dataset to tall format, with columns for the intestinal location and the teer value.
-   Note that we are giving you names_pattern

```{r, error=TRUE, eval=FALSE}
endo_data |> 
  pivot_longer(
    cols =  ,
    names_pattern = "(.+)_teer",
    names_to =   ,
    values_to = 
  )
```

### Solution

-   Fill in the blanks to pivot this dataset to tall format, with columns for the intestinal location and the teer value.

```{r, error=TRUE, eval=FALSE}
endo_data |> 
  pivot_longer(
    cols = "duod_teer":"colon_teer",
    names_pattern = "(.+)_teer",
    names_to = c("location"),
    values_to = "teer"
  )
```

-   Run the code, and look at the resulting table.

### Result

```{r, echo=FALSE}
endo_data |> 
  pivot_longer(
    cols = "duod_teer":"colon_teer",
    names_pattern = "(.+)_teer",
    names_to = c("location"),
    values_to = "teer"
  )
```
-   Do you think that portal hypertension has an effect on TEER and (its inverse) epithelial leakiness?

:::

## Exercise PH1

<br>

Complete Data Cleaning Fundamentals Exercise PH1.

-   If you have the exercise done correctly, click on the Reactions tab in Zoom, and click to put the "thumbs up" emoji `r knitr::asis_output("\U1F44D")` on your screen.

-   If you are having trouble, click on the Reactions tab in Zoom, and click to put the "raised hand" emoji `r knitr::asis_output("\U270B")` on your screen.

[--\> Take me to the exercises \<--](https://shannonpileggi.github.io/rmedicine-data-cleaning-2023/exercises.html){target="_blank"}

```{r}
#| echo: false
#| cache: false
countdown(minutes = 3, font_size = "4em")
```

## Pivoting Wider

::: panel-tabset

### Tall messy_uc Data
::: columns

::: {.column width="50%"}

-   Wide data is less common, but sometimes needed
-   Here we will convert the tall version of our selected messy_uc data back to wide.
-   This is what the tall data look like

:::


::: {.column width="50%"}

```{r, echo=FALSE}
tall
```

:::

:::

### Code to Pivot

```{r echo=TRUE, eval=FALSE}
tall |> 
  pivot_wider(
    id_cols = c(pat_id, treatment), # Variables not pivoted
    names_from = measure, # will become column names
    values_from = score # will become values
  )
```

### Wider Result

```{r echo=FALSE, eval=TRUE}
tall |> 
  pivot_wider(
    id_cols = c(pat_id, treatment),
    names_from = measure,
    values_from = score
  )
```

:::

## Fill in Unobserved Date-Times with padr

::: columns
::: {.column width="55%"}
```{r, echo=FALSE}
library(padr)
library(tidyverse)
emergency <- padr::emergency
head(emergency) |>  
  select(title, time_stamp)
```
:::

::: {.column width="45%"}
-   The **emergency** data set in the {padr} package contains \> 120K emergency calls from Montgomery County, PA over a period of \~ 11 months.
-   Each call has a title and a timestamp
:::
:::

## Thickening Time to a Usable Level

::: panel-tabset
### Goal

-   The thicken function adds a column to a data frame that is of a higher interval than the original variable.
-   The variable **time_stamp** has the interval of seconds
-   We can thicken the data to day, or to week, or to month.
-   Then we can count events by a usable unit of time

### Code

-   We will thicken to month
-   Then count overdoses by month

```{r}
#| echo: true
#| eval: false
emergency |> 
  thicken('month') |> 
  filter(str_detect(title, "OVERDOSE")) |>   group_by(time_stamp_month) |> 
  mutate(ods = sum(str_detect(title, "OVERDOSE"))) |> 
    select(time_stamp_month, ods) |> 
  distinct()
```

### Result

-   This lets us count events like overdoses by month with time_stamp_month.

```{r}
#| echo: false
#| eval: true
by_month <- emergency |> 
  thicken('month') |> 
  filter(str_detect(title, "OVERDOSE")) |>
  group_by(time_stamp_month) |> 
  mutate(ods = sum(str_detect(title, "OVERDOSE"))) |> 
  select(time_stamp_month, ods) |> 
  distinct()

by_month
```

### Plot Monthly 
```{r}
#| output-location: column-fragment
by_month |> 
  ggplot(aes(x = time_stamp_month,
             y = ods)) +
  geom_point(size = 3, color = "red") +
  geom_line() +
  labs(y = "Overdoses Per Month",
       x = "Month",
  title = "Monthly Overdose Calls") +
  theme_linedraw(base_size = 20) +
  ylim(0,175)
```

:::

## Padding unobserved dates (weekends?)

::: columns
::: {.column width="60%"}
-   The pad function allows you to fill in missing intervals.
-   As an example, my hospital only runs fecal calprotectin tests on weekdays.
-   This can lead to weird discontinuities in data over a weekend (Dec 3-4).
-   No observations on weekend days.
:::

::: {.column width="40%"}
```{r}
#| echo: false
fcp <- tibble::tribble(
  ~pat_id, ~date, ~fcp,
  '001', "12-01-2022", 1574,
  '001', "12-02-2022", 1323,
  '001', "12-05-2022", 673,
  '001', "12-06-2022", 314,
  '001', "12-07-2022", 168,
  '002', "11-30-2022", 1393,
  '002', "12-01-2022", 1014,
  '002', "12-02-2022", 812,
  '002', "12-05-2022", 247,
  '002', "12-06-2022", 118,
  '003', "12-02-2022", 987,
  '003', "12-05-2022", 438,
  '003', "12-06-2022", 312,
  '003', "12-05-2022", 194,
  '003', "12-06-2022", 101
) |> mutate(date = lubridate::mdy(date))

fcp
```
:::
:::

## Padding Unobserved Times

::: panel-tabset
### The Problem

-   We can fill in (pad) the unobserved weekend days with the **pad()** function.

### The Code

```{r}
#| echo: true
#| eval: false
fcp |> 
  pad(group = "pat_id") |> 
  tidyr::fill(pat_id) |> # fill in the missing pat_ids
  print(n = 14)
```

### The Result

::: columns
::: {.column width="40%"}
```{r}
#| echo: false
#| eval: true
fcp |> 
  pad(group = "pat_id") |> 
  tidyr::fill(pat_id) |> 
  print(n = 14)
```
:::

::: {.column width="60%"}
-   New observations are created on the missing dates
-   NAs are filled in for the missing FCPs, with one for each day and group (pat_id)
-   we used **tidyr::fill(pat_id)** to fill in the missing pat_ids
:::
:::
:::

## Joins of data from different sources

-   We often collect data from different sources that we later want to join together for analysis
    -   Data from local Electronic Medical Record
    -   Data from the CDC
    -   Data from the US Census
-   External data can illuminate our understanding of our local patient data

## Local Demographics with CDC SVI data

::: panel-tabset
### The Problem

::: columns
::: {.column width="60%"}
-   We have 2 datasets, one local Demographics and Census Tract, and one from the CDC that has values for Social Vulnerability Index by Census Tract
-   We want to know the median SVI for the neighborhood of each patient
-   We need to left_join these datasets together by matching on the Census Tract
:::

::: {.column width="40%"}
![](images/joins.PNG)
:::
:::

### The Data

-   What is the common uniqid/key?

::: columns
::: {.column width="60%"}
-   demo

```{r}
#| echo: false
demo <- tibble::tribble(
  ~pat_id, ~name, ~htn, ~census_tract,
  '001', "Arthur Blankenship", 0, 26161404400,
  '002', "Britney Jonas", 0, 26161405100,
  '003', "Sally Davis", 1, 26161402100,
  '004', "Al Jones", 0, 26161403200,
  '005', "Gary Hamill", 1, 26161405200,
  '006', "Ken Bartoletti", 0, 26161404500,
  '007', "Ike Gerhold", 0, 26161405600,
  '008', "Tatiana Grant", 0, 26161404300,
  '009', "Antione Delacroix", 1, 26161405500,
)

demo
```
:::

::: {.column width="40%"}
-   cdc

```{r}
#| echo: false
cdc <- tibble::tribble(
  ~census_tract, ~svi,
  26161404400, 0.12,
  26161405100, 0.67,
  26161402100, 0.43,
  26161403200, 0.07,
  26161405200, 0.71,
  26161404500, 0.23,
  26161405600, 0.27,
  26161404300, 0.21,
  26161405500, 0.62
)

cdc
```
:::
:::

### The Code

-   Replace the generic arguments to the left_join function to join the demographic data (*demo*) on the left to add Social Vulnerability index (svi) from the *cdc* dataset on the RHS. Left join by census tract.

```{r, echo=TRUE, eval=FALSE}
left_join(dataset_x, dataset_y, by = "key")
```

### The Result

-   Is there an association between social vulnerability and hypertension?

```{r, echo=FALSE, eval=TRUE}
left_join(demo, cdc, by = "census_tract")
```
:::

## Patient Demographics with Lab results (Your Turn to Join)

::: columns
::: {.column width="40%"}
-   We have some basic Patient Demographics in one table

```{r}
#| echo: false
demo <- tibble::tribble(
  ~pat_id, ~name, ~age,
  '001', "Arthur Blankenship", 67,
  '002', "Britney Jonas", 23,
  '003', "Sally Davis", 63,
  '004', "Al Jones", 44,
  '005', "Gary Hamill", 38,
  '006', "Ken Bartoletti", 33,
  '007', "Ike Gerhold", 52,
  '008', "Tatiana Grant", 42,
  '009', "Antione Delacroix", 27,
)

demo
```
:::

::: {.column width="60%" .fragment}
and potassium levels and creatinine levels in 2 other tables

```{r}
#| echo: false
pot <- tibble::tribble(
  ~pat_id, ~k,
  '001', 3.2,
  '002', 3.7,
  '003', 4.2,
  '004', 4.4,
  '005', 4.1,
  '006', 4.0,
  '007', 3.6,
  '008', 4.2,
  '009', 4.9,
)

head(pot)
```

```{r}
#| echo: false
cr <- tibble::tribble(
  ~pat_id, ~cr,
  '001', 0.2,
  '002', 0.5,
  '003', 0.9,
  '004', 1.5,
  '005', 0.7,
  '006', 0.9,
  '007', 0.7,
  '008', 1.0,
  '009', 1.7,
)

head(cr)
```
:::
:::

## Your Turn to Join

-   We want to join the correct labs (9 rows each) to the correct patients.
-   The unique identifier (called the uniqid or key or recordID) is pat_id.
-   It only occurs once for each patient/row
-   It appears in each table we want to join
-   The pat_id is of the character type in each (a common downfall if one is character, one is numeric, but they **look** the same - but don't match)
-   We want to start with demographics, then add datasets that match to the right.
-   We will use *demo* as our base dataset on the left hand side (LHS), and first join the potassium (*pot*) results (RHS)

## What the Left Join Looks Like

   <div class="animations"><img alt="gif here" width="80%" height="80%" src="https://raw.githubusercontent.com/gadenbuie/tidyexplain/main/images/left-join.gif"> </div>  
   
## Your Turn to Join

::: panel-tabset
### The Problem

-   Joining *demo* to *pot* with a left_join
-   left_join(data_x, data_y, by = "uniqid")

### The Code

-   replace the generic arguments below with the correct ones to join *demo* to *pot* and produce *new_data*.

```{r, error=TRUE, eval=FALSE}
new_data <- left_join(data_x, data_y, by = "uniqid")
new_data
```

### The Result

```{r, echo=FALSE}
new_data <- left_join(demo, pot, by = "pat_id")
new_data
```

:::

## Exercise PH2

<br>

Complete Data Cleaning Fundamentals Exercise PH2.

[--\> Take me to the exercises \<--](https://shannonpileggi.github.io/rmedicine-data-cleaning-2023/exercises.html){target="_blank"}

-   If you have the exercise done correctly, click on the Reactions tab in Zoom, and click to put the "thumbs up" emoji `r knitr::asis_output("\U1F44D")` on your screen.

-   If you are having trouble, click on the Reactions tab in Zoom, and click to put the "raised hand" emoji `r knitr::asis_output("\U270B")` on your screen.

```{r}
#| echo: false
#| cache: false
countdown(minutes = 3, font_size = "4em")
```

## Solution PH2

```{r, echo=TRUE, eval=FALSE}
new_data <- left_join(demo, pot, by = "pat_id")
new_data
```

## Now add Creatinine (cr) to new_data

::: panel-tabset
### The Problem

-   Joining new_data and cr with a left_join
-   left_join(data_x, data_y, by = "uniqid")

### The Code

-   Replace the generic arguments with the correct ones to join new_data and cr and produce new_data2.

```{r, error=TRUE, eval=FALSE}
new_data2 <- left_join(data_x, data_y, by = "uniqid")
new_data2
```

### The Result

```{r, echo=FALSE}
new_data2 <- left_join(new_data, cr, by = "pat_id")
new_data2
```
- Al has HTN and DM2
- Antione has early stage FSGS
:::

## Exercise PH3

<br>

Complete Data Cleaning Fundamentals Exercise PH3.

[--\> Take me to the exercises \<--](https://shannonpileggi.github.io/rmedicine-data-cleaning-2023/exercises.html){target="_blank"}

-   If you have the exercise done correctly, click on the Reactions tab in Zoom, and click to put the "thumbs up" emoji `r knitr::asis_output("\U1F44D")` on your screen.

-   If you are having trouble, click on the Reactions tab in Zoom, and click to put the "raise hand" emoji `r knitr::asis_output("\U270B")` on your screen.

```{r}
#| echo: false
#| cache: false
countdown(minutes = 3, font_size = "4em")
```

## Workhorse Joins

-   *left_join* is your workhorse. Start with patient identifiers/uniqid and add data to the right side.
-   Sometimes you will need to wrangle/process incoming data, then *right_join* it to the patient demographics

::: columns

::: {.column width="50%"}
   <div class="animations"><img alt="gif here" width="100%" height="100%" src="https://raw.githubusercontent.com/gadenbuie/tidyexplain/main/images/left-join.gif"> </div>  
:::

::: {.column width="50%"}
   <div class="animations"><img alt="gif here" width="100%" height="100%" src="https://raw.githubusercontent.com/gadenbuie/tidyexplain/main/images/right-join.gif"> </div>  
:::

:::

## Fancy Joins

-   There are multiple kinds of fancy joins (semi_join, anti_join, inner_join, full_join, union, intersect, setdiff) which come in handy once in a while.
-   These are all well explained [here](http://statseducation.com/Introduction-to-R/modules/tidy%20data/joins/#:~:text=semi_join(x%2C%20y)%3A%20Return,This%20is%20a%20filtering%20join.)

::: columns

::: {.column width="50%"}

 <div class="animations"><img alt="gif here" width="100%" height="100%" src="https://raw.githubusercontent.com/gadenbuie/tidyexplain/main/images/semi-join.gif"> </div>  
:::

::: {.column width="50%"}
 <div class="animations"><img alt="gif here" width="100%" height="100%" src="https://raw.githubusercontent.com/gadenbuie/tidyexplain/main/images/anti-join.gif"> </div>  
:::

:::

## Data Horror

::: columns

::: {.column width="50%"}
Data So Messy, it Constitutes a Data Crime

<br>
![Messy](images/messy.png)

<br>
What Should You Do?
:::

::: {.column width="50%"}
<div class="tenor-gif-embed" data-postid="21508179" data-share-method="host" data-aspect-ratio="1.33333" data-width="100%"><a href="https://tenor.com/view/apocalypse-now-marlon-brando-colonel-kurtz-kurtz-the-horror-gif-21508179">Apocalypse Now Marlon Brando GIF</a>from <a href="https://tenor.com/search/apocalypse+now-gifs">Apocalypse Now GIFs</a></div> <script type="text/javascript" async src="https://tenor.com/embed.js"></script>
:::

:::


## Two Options

::: columns

::: {.column width="50%"}
1. You can find the person who collected/entered the data

- If you don't correct this behavior now, this will torture _**many**_ future data analysts
- Educate in this *teachable moment* about tidy data
- **Improve** the world
- Send them to Tidy Spreadsheets on YouTube [here](https://www.youtube.com/watch?v=9f-hpJbjKZo)
:::

::: {.column width="50%"}
2. There is no way to find the person who collected/entered the data

- Pull out some advanced data cleaning packages made for this mess
  - unpivotr
  - tidyxl
  - unheadr
- Lean on a free e-book, [Spreadsheet Munging Strategies](https://nacnudus.github.io/spreadsheet-munging-strategies/)
- Watch the 14 min video [here](https://www.youtube.com/watch?v=ShWxAqnY2YE)
:::

:::
