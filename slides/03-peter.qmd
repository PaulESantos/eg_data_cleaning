# Wrangling your Data

```{r}
#| echo: false
#| message: false
library(readxl)
library(tidyverse)
library(janitor)
library(here)
library(gtsummary)
library(padr)
library(countdown)
```
## Deciding on the Unit of Observation

-   In Prospective Data Collection, a patient arrives for a visit. All data collected are part of the same observation/visit.
    -   Alternatively, we can divide a visit into distinct observations, like blood pressure, a PHQ-9 depression questionnaire, and a hemoglobin measurement (all on the same date)
-   In retrospective data review, we can divide the observations up as we choose. For inpatient stays, we need to decide *a priori* on how to handle multiple observations of the same type (e.g., vitals q6h) in the same day.
    -   use the 0800 observation each day?
    -   use the daily average?
    -   use the max values each day?

## What is the Unit of Analysis?

-   While the unit of observation may be straightforward for outpatient visits, and complicated for inpatient stays, in the end we need to select a Unit of Analysis
-   This Unit of Analysis usually depends on the **Question** we want to ask

## Is the Unit of Analysis the Patient?

-   Did the patient die?
-   Did the patient have the outcome of colectomy?
-   Did the patient reach disease remission?

## Is the Unit of Analysis the Visit/Encounter?

-   Often these are within-patient outcomes
    -   Did the C-reactive protein improve from Week 0 to Week 8?
    -   Did the number of sickle cell crises/year decrease after CRISPR gene therapy?
    -   Did the endoscopic ulcer score decrease on the experimental therapy vs placebo?

## Reshaping your data with tidyr

-   We often enter data by patient
-   Spreadsheets encourage us to enter longitudinal data as long rows (per patient)
-   We end up with wide data

\[Image\]

## Reshaping your data with tidyr

-   R (and most R functions) are vectorized to handle tall data
-   One small observation per row
-   Lots of analyses in R are easier with tall data

\[Image\]

## Pivoting Longer (common)

-   We need to 'pivot' data from wide to tall on the regular
-   This "lengthens" data, increasing the number of rows, and decreasing the number of columns
-   We will be looking at Visit Dates/Measures

## Pivoting Longer

-   Arguments: data, cols, names_to, values_to, and many optional arguments
-   Details from the tidyverse help page are [here](https://tidyr.tidyverse.org/reference/pivot_longer.html)
-   **data** is your dataframe/tibble - you can pipe this in
-   **cols** = columns to pivot, as a vector of names, or by number, or selected with [tidyselect](https://tidyselect.r-lib.org) functions
-   **names_to** = A character vector specifying the new column or columns to create from the information stored in the column names of data specified by cols.
-   **values_to** = A string specifying the name of the column to create from the data stored in cell values.

## Pivoting Longer (Example)

Let's start with the wide version (selected columns from messy_uc)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
wide <- readxl::read_excel(here("data/messy_uc.xlsx"), sheet = 3, skip = 6) |>
  janitor::clean_names() |>
  janitor::remove_empty(which = c("rows", "cols")) |> 
  select(pat_id, treatment, ends_with("mes"), ends_with("bss"), ends_with("emo")) |> 
  mutate(across(3:8, as.numeric)) 

wide
```

-    Note that there are 30 rows, one per patient, with 6 measured quantities for each patient.

## Pivoting Longer (Example)

This is the tall version we want to end up with.

```{r, echo=FALSE}
wide |> 
  pivot_longer(cols = 3:8, 
               names_to = "measure",
               values_to = "score") 
```

-    Note that there now 180 rows (30\*6), with one row per observation measure.

## Doing the pivot_longer()

What values do we want for these key arguments in order to pivot_longer?

-   cols (which columns to pivot)
-   names_to (variable to store the names)
-   values_to (variable to store the values)

```{r, echo=FALSE}
wide
```

## Pivoting Longer In Action

::: panel-tabset
### Problem: Wide

```{r, echo=FALSE}
wide
```

### Code: pivot_longer

```{r, echo=TRUE, eval=FALSE}
wide |> 
  pivot_longer(
    cols = "start_mes":"end_emo", 
    # could also use 3:8
               names_to = "measure",
               values_to = "score")
```

### Result: Tall

```{r, echo=FALSE, eval=TRUE}
tall <- wide |> 
  pivot_longer(
    cols = "start_mes":"end_emo", 
               names_to = "measure",
               values_to = "score")

tall
```
:::

## One Minor Issue - Separation of measure

::: panel-tabset
### Problem

-   the "measure" column combines a timepoint and the measure
-   Needs to be separated.
-   You already know how to use *separate()*
-   Arguments
    -   col
    -   sep
    -   into

### Code

```{r, echo=TRUE, eval=FALSE}
tall |> 
  separate(col = "measure",
           sep = "_",
           into = c("timept", "measure"))
```

### Result

```{r, echo=FALSE, eval=TRUE}
tall |> 
  separate(col = "measure",
           sep = "_",
           into = c("timept", "measure"))
```

### Alternative within pivot_longer

- You can do this _within_ pivot_longer with one more argument (if you are _fancy_)

```{r}
wide |> 
  pivot_longer(cols = 3:8,
    names_to = c("timept", "measure"),
    names_sep = "_",
    values_to = "score")

```

:::

## Pivoting Longer

-   Your Turn (Exercise) with endo_data
- Measurements of Trans-Epithelial Electrical Resistance (TEER, the inverse of leakiness) were taken from biopsies of 3 segments of intestine.
- This might be affected by portal hypertension in patients with liver cirrhosis

```{r, echo=FALSE}
endo_data <- tibble::tribble(
  ~pat_id, ~portal_htn, ~duod_teer, ~ileal_teer, ~colon_teer,
  001, 1, 4.33, 14.57, 16.23,
  002, 0, 11.67, 15.99, 18.97,
  003, 1, 4.12, 13.77, 15.22,
  004, 1, 4.62, 16.37, 18.12,
  005, 0, 12.43, 15.84, 19.04,
  006, 0, 13.05, 16.23, 18.81,
  007, 0, 11.88, 15.72, 18.31,
  008, 1, 4.87, 16.59, 18.77,
  009, 1, 4.23, 15.04, 16.87,
  010, 0, 12.77, 16.73, 19.12
)
endo_data
```


## Pivoting Longer with endo_data

::: panel-tabset

### The Dataset

```{r, echo=FALSE}
endo_data
```

### The Arguments

- What values do you want to use for:
  - cols
  - names_pattern = "(.+)_teer"
  - names_to
  - values_to
- Note that we are giving you the value for names_pattern, which means that we want to keep the characters of the name (of whatever length) before "_teer"

### The Code

- Fill in the blanks to pivot this dataset to tall format, with columns for the intestinal location and the teer value.
- Note that we are giving you names_pattern

```{r, error=TRUE, eval=FALSE}
endo_data |> 
  pivot_longer(
    cols =  ,
    names_pattern = "(.+)_teer",
    names_to =   ,
    values_to = 
  )
```

### The Solution

- Fill in the blanks to pivot this dataset to tall format, with columns for the intestinal location and the teer value.

```{r, error=TRUE, eval=FALSE}
endo_data |> 
  pivot_longer(
    cols = "duod_teer":"colon_teer",
    names_pattern = "(.+)_teer",
    names_to = c("location"),
    values_to = "teer"
  )
```

- Run the code, and look at the resulting table.
- Do you think that portal hypertension has an effect on TEER and (its inverse) epithelial leakiness?

:::

## Exercise PH1


<br>

Complete Data Cleaning Fundamentals Exercise PH1.

- If you have the exercise done correctly, click on the Reactions tab in Zoom, and click to put the "thumbs up" emoji on your screen.

- If you are having trouble, click on the Reactions tab in Zoom, and click to put the "raise hand" emoji on your screen.

[--> Take me to the exercises <--](https://shannonpileggi.github.io/rmedicine-data-cleaning-2023/exercises.html){target="_blank"}

```{r}
#| echo: false
#| cache: false
countdown(minutes = 3, font_size = "4em")
```

## Pivoting Wider

::: panel-tabset

### Tall messy_uc Data
-   Wide data is less common, but sometimes needed
- Here we will convert the tall version of our selected messy_uc data back to wide.
- This is what the tall data look like

```{r, echo=FALSE}
tall
```

### Code

```{r echo=TRUE, eval=FALSE}
tall |> 
  pivot_wider(
    id_cols = c(pat_id, treatment), # Variables not pivoted
    names_from = measure, # will become column names
    values_from = score # will become values
  )
```

### Wider Result

```{r echo=FALSE, eval=TRUE}
tall |> 
  pivot_wider(
    id_cols = c(pat_id, treatment),
    names_from = measure,
    values_from = score
  )
```

:::

# Filling in Unobserved Dates/Times with padr

::: columns
::: {.column width="55%"}

```{r, echo=FALSE}
library(padr)
library(tidyverse)
emergency <- padr::emergency
head(emergency) |>  
  select(title, time_stamp)
```

:::

::: {.column width="45%"}
- The **emergency** data set in the {padr} package contains > 120K emergency calls from Montgomery County, PA over a period of ~ 11 months.
- Each call has a title and a timestamp

:::

:::

## Thickening Time to a Usable Level

::: panel-tabset

### Goal
- The thicken function adds a column to a data frame that is of a higher interval than the original variable.
- The variable **time_stamp** has the interval of seconds
- We can thicken the data to day, or to week, or to month.
- Then we can count events by a usable unit of time

### Code
- We will thicken to month
- Then count overdoses by month

```{r}
#| echo: true
#| eval: false
emergency |> 
  thicken('month') |> 
  filter(str_detect(title, "OVERDOSE")) |>   group_by(time_stamp_month) |> 
  mutate(ods = sum(str_detect(title, "OVERDOSE"))) |> 
    select(time_stamp_month, ods) |> 
  distinct()
```

### Result
- This lets us count events like overdoses by month with time_stamp_month.

```{r}
#| echo: false
#| eval: true
emergency |> 
  thicken('month') |> 
  filter(str_detect(title, "OVERDOSE")) |>
  group_by(time_stamp_month) |> 
  mutate(ods = sum(str_detect(title, "OVERDOSE"))) |> 
  select(time_stamp_month, ods) |> 
  distinct()
```

:::

## Padding unobserved dates (weekends?)

::: columns
::: {.column width="60%"}

- The pad function allows you to fill in missing intervals.
- As an example, my hospital only runs fecal calprotectin tests on weekdays.
- This can lead to weird discontinuities in data over a weekend (Dec 3-4).
- No observations on weekend days.

:::

::: {.column width="40%"}

```{r}
#| echo: false
fcp <- tibble::tribble(
  ~pat_id, ~date, ~fcp,
  '001', "12-01-2022", 1574,
  '001', "12-02-2022", 1323,
  '001', "12-05-2022", 673,
  '001', "12-06-2022", 314,
  '001', "12-07-2022", 168,
  '002', "11-30-2022", 1393,
  '002', "12-01-2022", 1014,
  '002', "12-02-2022", 812,
  '002', "12-05-2022", 247,
  '002', "12-06-2022", 118,
  '003', "12-02-2022", 987,
  '003', "12-05-2022", 438,
  '003', "12-06-2022", 312,
  '003', "12-05-2022", 194,
  '003', "12-06-2022", 101
) |> mutate(date = lubridate::mdy(date))

fcp
```

:::

:::

## Padding Unobserved Times

::: panel-tabset

### The Problem

- We can fill in (pad) the unobserved weekend days with the **pad()** function.

### The Code

```{r}
#| echo: true
#| eval: false
fcp |> 
  pad(group = "pat_id") |> 
  tidyr::fill(pat_id) |> # fill in the missing pat_ids
  print(n = 14)
```

### The Result

::: columns

::: {.column width="40%"}

```{r}
#| echo: false
#| eval: true
fcp |> 
  pad(group = "pat_id") |> 
  tidyr::fill(pat_id) |> 
  print(n = 14)
```

:::

::: {.column width="60%"}

- New observations are created on the missing dates
- NAs are filled in for the missing FCPs, with one for each day and group (pat_id)
- we used **tidyr::fill(pat_id)** to fill in the missing pat_ids

:::

:::

:::

## Joins of data from different sources

- We often collect data from different sources that we later want to join together for analysis
  - Data from local Electronic Medical Record
  - Data from the CDC
  - Data from the US Census
- External data can illuminate our understanding of our local patient data

## Local Demographics with CDC SVI data 

::: panel-tabset

### The Problem

::: columns
::: {.column width="60%"}

- We have 2 datasets, one local Demographics and Census Tract, and one from the CDC that has values for Social Vulnerability Index by Census Tract
- We want to know the median SVI for the neighborhood of each patient
- We need to left_join these datasets together by matching on the Census Tract

:::

::: {.column width="40%"}

![](images/joins.PNG)



:::

:::



### The Data
- What is the common uniqid/key?

::: columns
::: {.column width="60%"}

- demo

```{r}
#| echo: false
demo <- tibble::tribble(
  ~pat_id, ~name, ~htn, ~census_tract,
  '001', "Arthur Blankenship", 0, 26161404400,
  '002', "Britney Jonas", 0, 26161405100,
  '003', "Sally Davis", 1, 26161402100,
  '004', "Al Jones", 0, 26161403200,
  '005', "Gary Hamill", 1, 26161405200,
  '006', "Ken Bartoletti", 0, 26161404500,
  '007', "Ike Gerhold", 0, 26161405600,
  '008', "Tatiana Grant", 0, 26161404300,
  '009', "Antione Delacroix", 1, 26161405500,
)

demo
```

:::

::: {.column width="40%"}

- cdc

```{r}
#| echo: false
cdc <- tibble::tribble(
  ~census_tract, ~svi,
  26161404400, 0.12,
  26161405100, 0.67,
  26161402100, 0.43,
  26161403200, 0.07,
  26161405200, 0.71,
  26161404500, 0.23,
  26161405600, 0.27,
  26161404300, 0.21,
  26161405500, 0.62
)

cdc
```

:::

:::

### The Code
- Replace the arguments to join the demographic data (*demo*) on the left to add Social Vulnerability index (svi) from the *cdc* dataset on the RHS. Left join by census tract.

```{r, echo=TRUE, eval=FALSE}
left_join(dataset_x, dataset_y, by = "key")
```


### The Result

- Is there an association between social vulnerability and hypertension?

```{r, echo=FALSE, eval=TRUE}
left_join(demo, cdc, by = "census_tract")
```

:::

## Patient Demographics with Lab results (Your Turn to Join)

::: {.columns}
::: {.column width="40%"}

- We have some basic Patient Demographics in one table

```{r}
#| echo: false
demo <- tibble::tribble(
  ~pat_id, ~name, ~age,
  '001', "Arthur Blankenship", 67,
  '002', "Britney Jonas", 23,
  '003', "Sally Davis", 63,
  '004', "Al Jones", 44,
  '005', "Gary Hamill", 38,
  '006', "Ken Bartoletti", 33,
  '007', "Ike Gerhold", 52,
  '008', "Tatiana Grant", 42,
  '009', "Antione Delacroix", 27,
)

demo
```
:::

::: {.column width="60%"}
and potassium levels and creatinine levels in 2 other tables

```{r}
#| echo: false
pot <- tibble::tribble(
  ~pat_id, ~k,
  '001', 3.2,
  '002', 3.7,
  '003', 4.2,
  '004', 4.4,
  '005', 4.1,
  '006', 4.0,
  '007', 3.6,
  '008', 4.2,
  '009', 4.9,
)

head(pot)
```

```{r}
#| echo: false
cr <- tibble::tribble(
  ~pat_id, ~cr,
  '001', 0.2,
  '002', 0.5,
  '003', 0.9,
  '004', 1.5,
  '005', 0.7,
  '006', 0.9,
  '007', 0.7,
  '008', 1.0,
  '009', 1.7,
)

head(cr)
```

:::

:::

## Your Turn to Join

- We want to join the correct labs (9 rows each) to the correct patients.
- The unique identifier (called the uniqid or key or recordID) is pat_id. 
- It only occurs once for each patient/row
- It appears in each table we want to join
- The pat_id is of the character type in each (a common downfall if one is character, one is numeric, but they **look** the same - but don't match)
- We want to start with demographics, then add datasets that match to the right.
- We will use *demo* as our base dataset on the left hand side (LHS), and first join the potassium (*pot*) results (RHS)

## Your Turn to Join

::: panel-tabset

### The Problem
- Joining _demo_ to _pot_ with a left_join
- left_join(data_x, data_y, by = "uniqid")

### The Code
- replace the arguments with the correct ones to join _demo_ to _pot_ and produce *new_data*.

```{r, error=TRUE, eval=FALSE}
new_data <- left_join(data_x, data_y, by = "uniqid")
new_data
```

### The Result


```{r, echo=FALSE}
new_data <- left_join(demo, pot, by = "pat_id")
new_data
```

:::

## Exercise PH2


<br>

Complete Data Cleaning Fundamentals Exercise PH2.

[--> Take me to the exercises <--](https://shannonpileggi.github.io/rmedicine-data-cleaning-2023/exercises.html){target="_blank"}

- If you have the exercise done correctly, click on the Reactions tab in Zoom, and click to put the "thumbs up" emoji on your screen.

- If you are having trouble, click on the Reactions tab in Zoom, and click to put the "raise hand" emoji on your screen.

```{r}
#| echo: false
#| cache: false
countdown(minutes = 3, font_size = "4em")
```

## Solution PH2

```{r, echo=TRUE, eval=FALSE}
new_data <- left_join(demo, pot, by = "pat_id")
new_data
```

## Now add Creatinine (cr) to new_data

::: panel-tabset

### The Problem
- Joining new_data and cr with a left_join
- left_join(data_x, data_y, by = "uniqid")

### The Code
- Replace the arguments with the correct ones to join new_data and cr and produce new_data2.

```{r, error=TRUE, eval=FALSE}
new_data2 <- left_join(data_x, data_y, by = "uniqid")
new_data2
```

### The Result


```{r, echo=FALSE}
new_data2 <- left_join(new_data, cr, by = "pat_id")
new_data2
```

:::

## Exercise PH3


<br>

Complete Data Cleaning Fundamentals Exercise PH3.

[--> Take me to the exercises <--](https://shannonpileggi.github.io/rmedicine-data-cleaning-2023/exercises.html){target="_blank"}

- If you have the exercise done correctly, click on the Reactions tab in Zoom, and click to put the "thumbs up" emoji on your screen.

- If you are having trouble, click on the Reactions tab in Zoom, and click to put the "raise hand" emoji on your screen.

```{r}
#| echo: false
#| cache: false
countdown(minutes = 3, font_size = "4em")
```

## Other Fancy Joins
- *left_join* is your workhorse. Start with patient identifiers/uniqid and add data to the right side.
- Sometimes you will need to wrangle/process incoming data, then *right_join* it to the patient demographics
- There are multiple kinds of fancy joins (semi_join, anti_join, inner_join, full_join) which come in handy once in a while.+
- These are all well explained [here](http://statseducation.com/Introduction-to-R/modules/tidy%20data/joins/#:~:text=semi_join(x%2C%20y)%3A%20Return,This%20is%20a%20filtering%20join.)
